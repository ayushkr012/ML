{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# importing libraries\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('dataSetName.csv')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # View the data types and non-null values in the dataset -->\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with the mean\n",
    "df_filled = df.fillna(df.mean())\n",
    "\n",
    "# # Display information about missing values after filling\n",
    "print(\"\\nMissing values after filling:\")\n",
    "print(df_filled.isnull().sum())\n",
    "\n",
    "# # Save the filled dataset to a new CSV file\n",
    "df_filled.to_csv('newDataSetName.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about missing values after filling\n",
    "print(\"\\nMissing values after filling:\")\n",
    "print(df_filled.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(df.duplicated().sum())\n",
    "# remove any duplicate rows\n",
    "df.drop_duplicates(keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# : Statistical Analysis descriptive overview of the dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting features (X) and target variable (y)\n",
    "X = df_filled.drop(columns=['y'])  # Assuming 'y' is your target variable\n",
    "y = df_filled['y']\n",
    "\n",
    "# In this code snippet:\n",
    "\n",
    "# - `df_filled.drop(columns=['y'])` drops the 'y' column from the DataFrame, leaving only the features.\n",
    "# - `df_filled['y']` selects the 'y' column, which represents your target variable.\n",
    "\n",
    "# Make sure to replace `'y'` with the actual label of your target variable if it's different in your dataset.\n",
    "\n",
    "# Now, you have your features stored in `X` and your target variable stored in `y`, which you can use for further analysis or modeling, such as linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Assuming you have your preprocessed dataset, X and y, where X is your feature matrix and y is your target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the features (optional but recommended for linear regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and fitting the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "train_score = model.score(X_train_scaled, y_train)\n",
    "test_score = model.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training R^2 score:\", train_score)\n",
    "print(\"Testing R^2 score:\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, you can print the coefficients and intercept of the linear regression model\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In this example:\n",
    "# 1. We split the dataset into training and testing sets using `train_test_split`.\n",
    "# 2. We standardized the features using `StandardScaler` to ensure that each feature contributes equally to the computation of the regression model (optional but recommended).\n",
    "# 3. We created an instance of `LinearRegression`, fit the model to the training data, and evaluated it using R^2 score.\n",
    "# 4. Finally, we printed out the coefficients and intercept of the linear regression model.\n",
    "\n",
    "# Make sure to replace `X` and `y` with your preprocessed feature matrix and target variable, respectively. Additionally, you might need to adjust parameters like `test_size` in `train_test_split` based on your dataset size and requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
